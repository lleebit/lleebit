import pandas as pd
text_ana = pd.read_csv('《消失的她》豆瓣短评数据text = list(text_ana['标题/微博内容'])
text.csv', index_col = 0)
# text_ana = text_ana.drop(['Unnamed: 0'], axis=1)
text_ana
text = list(text_ana['评论'])
text
import jieba

def word_segmentation(text):
    words = jieba.lcut(text)
    return ' '.join(words)

text_ana['评论'] = text_ana['评论'].astype(str).apply(word_segmentation)
text_ana['评论'].head()
text = list(text_ana['评论'].dropna())
text
from sklearn.feature_extraction.text import CountVectorizer

# 初始化CountVectorizer
vectorizer = CountVectorizer()

# 将文本数据转换为词袋特征矩阵
X = vectorizer.fit_transform(text)

# 展示特征矩阵的形状
print('词袋特征矩阵的形状:', X.shape)
# 展示词袋模型中的所有特征词
print('特征词列表:', vectorizer.get_feature_names_out())
list(X.toarray())
# 获取词汇表
feature_words = vectorizer.get_feature_names_out()

# 计算词频统计
word_freq = dict(zip(feature_words, X.sum(axis=0).A1))
# 词袋模型中不重复的词汇的数量
len(word_freq)
# 观察词袋模型的词频计数结果
sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
